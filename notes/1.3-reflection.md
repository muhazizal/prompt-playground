TL;DR: LLMs are probabilistic pattern-matchers; “attention” is a dynamic spotlight over your context; prompt design feels like debugging thought—clarify intent, constrain outputs, and iterate with examples.

### Most Surprising

- Small wording/order shifts behavior dramatically; a last-line instruction can override earlier context.
- Temperature is a true distribution knob; multi-sample runs reveal a spectrum of plausible answers.
- Confidence ≠ correctness; vague prompts induce hallucinations even when outputs sound authoritative.
- Models obey formats strongly if you specify them; JSON schemas and few-shot examples act like “types” and tests.
- Tokens and latency trade-offs matter; longer prompts cost more and don’t always improve quality.

### Explaining Attention

- Think of attention as a spotlight that allocates focus across the words in your prompt at each step.
- Every token “votes” on which other tokens matter right now; attention weights decide who wins that influence.
- The context window is working memory; attention helps retrieve the most relevant bits from anywhere in it.
- Multi-head attention is many spotlights at once—some track structure, others numbers, names, or long-range links.
- It’s not understanding like a human; it’s weighted referencing of patterns learned during training.

### Prompt Design = Debugging Thought

- Decompose tasks: ask for steps, then the answer; it reduces leaps and errors.
- Constrain outputs: specify schema, fields, ranges, and “don’t guess” to reduce ambiguity/hallucinations.
- Calibrate with few-shot examples; they act like unit tests that shape the model’s “mental frame”.
- Set clear objectives and evaluation criteria; treat outputs like functions with expected properties.
- Iterate with evidence: inspect failures (format drift, factual slips) and adjust instructions, examples, or temperature.

### Practical Tips (from the Playground)

- Use multi-sample outputs and multiple temperatures to see variability; pick the best by majority or simple heuristics.
- Keep defaults sane (e.g., temperatures = [0.5] , samples = 2–3 ) and only expand when exploring edge cases.
- Make format the first-class requirement: start with structure (JSON/table), then content.
- Log token usage and latency; trim prompts ruthlessly and prefer concise, unambiguous instructions.
- When stakes are high, ask for sources or confidence tagging and penalize unsupported claims (“If uncertain, say so”).
