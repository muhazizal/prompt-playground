# Reflection

## 1. What’s the cost per call (tokens × price)?

Cost depends on the model and the number of input/output tokens.

- **Summarization (`gpt-4o-mini`):**
  - Input: ~$0.15 / 1M tokens
  - Output: ~$0.60 / 1M tokens
- **Embeddings (`text-embedding-3-small`):**
  - ~$0.02 / 1M tokens

A typical 1,000-token note summarization might cost a fraction of a cent. The `usage` object returned by the API provides the exact token counts for each call.

## 2. How could you cache results locally?

The project already implements a robust local caching strategy for embeddings to reduce costs and latency.

- **Embeddings Cache:**
  - An in-memory cache (`Map`) provides the fastest lookups for repeated requests within the same session.
  - A disk cache (`cache/embeddings.json`) persists embeddings across server restarts. Text content is hashed (`sha1`) to create a stable cache key.
  - The cache is automatically loaded on the first request and saved to disk with a debounce mechanism to avoid excessive writes.

- **Summary Caching:**
  - While not currently implemented, summaries could be cached using a similar file-based approach. A hash of the note content could serve as the cache key.

